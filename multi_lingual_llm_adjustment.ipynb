{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch accelerate transformers bitsandbytes \\\n",
        "             langchain langchain-core langchain-community langchain-classic \\\n",
        "             pypdf faiss-cpu sentence-transformers"
      ],
      "metadata": {
        "id": "IFPITwIBqLiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('colabllm')"
      ],
      "metadata": {
        "id": "wAxaTyoj0R6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-text-splitters"
      ],
      "metadata": {
        "id": "G9-bd8zl2biv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "# Corrected import for legacy chain:\n",
        "from langchain_classic.chains.retrieval_qa.base import RetrievalQA"
      ],
      "metadata": {
        "id": "O3R022dS2FaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes"
      ],
      "metadata": {
        "id": "0y2pKa9SkUJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "id": "W-w-qKgpkYC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# --- 1. Hugging Face Authentication (CRITICAL) ---\n",
        "HF_TOKEN = userdata.get('colabllm')\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n",
        "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = HF_TOKEN # <-- This line we just fixed\n",
        "\n",
        "# --- 2. Configuration Variables ---\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# 3. 4-bit Quantization Configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "print(\"âœ… Authentication, model_id, and bnb_config variables are defined.\")"
      ],
      "metadata": {
        "id": "NU8vAxMqj7mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "3edt5bWQ65P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"Please select your PDF file using the dialog box that appears below:\")\n",
        "\n",
        "# This opens the upload widget. 'uploaded' is a dictionary\n",
        "# where the keys are the filenames.\n",
        "uploaded = files.upload()\n",
        "\n",
        "# We assume you only uploaded one file (the PDF).\n",
        "# We take the first (and only) key in the dictionary to get the filename.\n",
        "try:\n",
        "    # Get the name of the file that was uploaded\n",
        "    uploaded_filename = list(uploaded.keys())[0]\n",
        "\n",
        "    # Define the official PDF_PATH variable\n",
        "    PDF_PATH = uploaded_filename\n",
        "\n",
        "    print(f\"\\nâœ… File successfully uploaded and path defined as: {PDF_PATH}\")\n",
        "\n",
        "except IndexError:\n",
        "    # Handles the case where the user closes the dialog without selecting a file\n",
        "    print(\"\\nâŒ No file was selected. Please run the cell again and choose a PDF.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ An error occurred during upload: {e}\")"
      ],
      "metadata": {
        "id": "pg_bhvEPEG9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Rerun the FAISS Creation using the new PDF_PATH variable ---\n",
        "# (You will need to ensure the necessary imports for these components are in the previous cells)\n",
        "\n",
        "loader = PyPDFLoader(PDF_PATH)\n",
        "data = loader.load()\n",
        "\n",
        "# Split the document into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(data)\n",
        "\n",
        "# Define the local HuggingFace Embeddings model\n",
        "embedding_model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_id)\n",
        "\n",
        "# Defines the required 'db' variable (FAISS Vector Store)\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "print(\"âœ… FAISS ('db') created successfully.\")"
      ],
      "metadata": {
        "id": "BAl4-vzaESyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# 1. Define Quantization Config (bnb_config)\n",
        "# This step is crucial for 4-bit inference on your GPU.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# 2. Load the Model and Tokenizer\n",
        "# This is where the 'model' variable is defined!\n",
        "print(f\"Loading Model: {model_id}...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"âœ… LLM Model and Tokenizer loaded successfully.\")"
      ],
      "metadata": {
        "id": "4GpD5XtTCsh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Define the Retriever ---\n",
        "# Converts the FAISS Vector Store ('db') into a searchable retriever object.\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "# --- 2. Create the QA Chain ---\n",
        "# Create a Hugging Face pipeline for text generation\n",
        "pipeline_instance = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    max_new_tokens=512 # Adjust as needed\n",
        ")\n",
        "\n",
        "# Wrap the Hugging Face pipeline with LangChain's HuggingFacePipeline\n",
        "llm_pipeline = HuggingFacePipeline(pipeline=pipeline_instance)\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm_pipeline,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        ")\n",
        "\n",
        "# --- 3. Define Query and Run Chain (This line defines 'result') ---\n",
        "query = \"What factors are explicitly excluded from consideration when employing a person, according to the NestlÃ© principles?\"\n",
        "print(f\"\\nâ“ Querying LLM with the context-relevant question: {query}\")\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "# --- 4. Print Result & Sources ---\n",
        "print(\"\\n--- RAG Chain Result ---\")\n",
        "print(result['result'])\n",
        "\n",
        "# Verify and display the retrieved sources\n",
        "print(\"\\n--- Source Documents ---\")\n",
        "for doc in result['source_documents']:\n",
        "    # Use .get() in case the metadata key is missing\n",
        "    print(f\"Source: {doc.metadata.get('source', 'N/A')} (Page: {doc.metadata.get('page', 'N/A')})\")\n",
        "    print(f\"Page Content Snippet: {doc.page_content[:200]}...\\n\")\n",
        "\n",
        "print(\"âœ… Block 3: RAG Chain executed successfully!\")"
      ],
      "metadata": {
        "id": "CApC_G0REu3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a local directory to hold the index files\n",
        "FAISS_PATH = \"faiss_index_nestle\"\n",
        "os.makedirs(FAISS_PATH, exist_ok=True)\n",
        "\n",
        "# Save the index and the mapping from index to document content\n",
        "db.save_local(FAISS_PATH)\n",
        "\n",
        "print(f\"âœ… FAISS index saved successfully to: /{FAISS_PATH}\")"
      ],
      "metadata": {
        "id": "M0FmfR91S2go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "-95gf4NkKZPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch accelerate transformers bitsandbytes --upgrade"
      ],
      "metadata": {
        "id": "N9PTjZUUGT13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "id": "-tluHFuBGl0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# multilingual model\n",
        "# BGE-M3 supports over 100 languages, including English and German\n",
        "# Good and solid open source choice for rag\n",
        "\n",
        "MULTILINGUAL_MODEL_NAME = \"BAAI/bge-m3\"\n",
        "\n",
        "# model setup arguments\n",
        "model_kwargs = {'device': 'cuda'} # for my T4 GPU setup\n",
        "encode_kwargs = {'normalize_embeddings': True}\n",
        "\n",
        "# Initialize the new multilingual embedding model\n",
        "print(f\"Loading new multilingual embedding model: {MULTILINGUAL_MODEL_NAME}...\")\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=MULTILINGUAL_MODEL_NAME,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "print(\"Mulitlingual model loaded The 'embeddings' variable is now defined.\")"
      ],
      "metadata": {
        "id": "Uydlq4la_LJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "print(\"Rebuilding FAISS Index with BAAI/bge-m3 embeddings...\")\n",
        "# this will be able to use new 'embeddings' model to create index\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "print(\"FAISS Index rebuilt successfully.\")\n",
        "\n",
        "# now im saving new index to disk\n",
        "FAISS_PATH = \"faiss_index_nestle\"\n",
        "db.save_local(FAISS_PATH)\n",
        "print(f\"New multilingual FAISS index saved to: {FAISS_PATH}\")"
      ],
      "metadata": {
        "id": "Evgq_8BRHMq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# 1. Delete the partially loaded model object (if it exists)\n",
        "try:\n",
        "    if 'model' in locals() or 'model' in globals():\n",
        "        del model\n",
        "    if 'tokenizer' in locals() or 'tokenizer' in globals(): # Added tokenizer\n",
        "        del tokenizer\n",
        "    if 'pipeline_instance' in locals() or 'pipeline_instance' in globals(): # Added pipeline_instance\n",
        "        del pipeline_instance\n",
        "    if 'llm_pipeline' in locals() or 'llm_pipeline' in globals(): # Added llm_pipeline\n",
        "        del llm_pipeline\n",
        "    if 'pipe' in locals() or 'pipe' in globals():\n",
        "        del pipe\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# 2. Force PyTorch to release its cached memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 3. Trigger Python's garbage collector\n",
        "gc.collect()\n",
        "\n",
        "print(\"âœ… GPU Memory cleared. Proceed to the next step.\")"
      ],
      "metadata": {
        "id": "L9dSxafVYUEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Configuration for 4-bit quantization (bnb_config should already be defined)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# ðŸ› ï¸ FIX: Define a memory map to explicitly allow offloading to the CPU/Disk\n",
        "max_memory_mapping = {\n",
        "    0: \"15GiB\", # Specify a concrete memory size for the GPU\n",
        "    \"cpu\": \"15GiB\", # Ensure you have enough CPU RAM for this\n",
        "}\n",
        "\n",
        "# Ensure the disk offload directory exists\n",
        "if not os.path.exists(\"/tmp/offload\"):\n",
        "    os.makedirs(\"/tmp/offload\")\n",
        "\n",
        "\n",
        "# Load the Model and Tokenizer\n",
        "print(f\"Reloading Mistral Model with Offloading: {model_id}...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    max_memory=max_memory_mapping, # Use the corrected max_memory_mapping\n",
        "    offload_folder=\"/tmp/offload\" # Specify offload folder separately for disk\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"âœ… LLM Model and Tokenizer loaded successfully. The RAG chain is ready to run.\")"
      ],
      "metadata": {
        "id": "tNc2x61UWKhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-core"
      ],
      "metadata": {
        "id": "8GxI0FRepZYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell first and wait for it to finish!\n",
        "!pip install \\\n",
        "    langchain \\\n",
        "    langchain-core \\\n",
        "    langchain-community \\\n",
        "    pypdf \\\n",
        "    faiss-cpu \\\n",
        "    sentence-transformers \\\n",
        "    torch \\\n",
        "    accelerate \\\n",
        "    transformers \\\n",
        "    bitsandbytes"
      ],
      "metadata": {
        "id": "my_dyVrApvqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forced german translation\n",
        "query = \"Was sind die Kernprinzipien der NestlÃ© HR-Richtlinie in Bezug auf die Leistung? Antworte auf Deutsch:\"\n",
        "print(f\"\\nRunning Query (Forcing German): {query}\")\n",
        "\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "print(\"\\n--- LLM ANSWER (German Output) ---\")\n",
        "print(result['result'])\n",
        "print(\"----------------------------------\")\n",
        "print(\"âœ… RAG Chain executed successfully!\")"
      ],
      "metadata": {
        "id": "O_cFmkuQtBva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MHc9r9HOx5ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0kmya5sl9JTs"
      }
    }
  ]
}